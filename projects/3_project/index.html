<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CoLA Kernels | Satyanarayana Chillale </title> <meta name="author" content="Satyanarayana Chillale"> <meta name="description" content="Optimized CUDA kernels for large-scale linear algebra"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://satyanarayanachillale.github.io/projects/3_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Satyanarayana</span> Chillale </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">CoLA Kernels</h1> <p class="post-description">Optimized CUDA kernels for large-scale linear algebra</p> </header> <article> <p><a href="https://satyachillale.github.io/assets/pdf/CoLA_kernels_report.pdf" rel="external nofollow noopener" target="_blank">[Project Paper]</a> <a href="https://satyachillale.github.io/assets/pdf/cola_presentation.pdf" rel="external nofollow noopener" target="_blank">[Presentation]</a> <a href="https://github.com/satyachillale/cola" rel="external nofollow noopener" target="_blank">[Code]</a></p> <p>CoLA, short for Compositional Linear Algebra, is a framework that tackles large-scale linear algebra tasks like matrix solves and eigenvalue problems in machine learning and scientific computing. While CoLA can automatically compose operations based on matrix structures (like sparsity or low-rank factorizations), its default Python-level implementations and dispatch mechanisms often lead to performance bottlenecks on GPUs. Below is a technical overview of how custom fused CUDA kernels can alleviate these bottlenecks and speed up select operations such as Arnoldi iterations, Cholesky decomposition, trace estimation, and SVD.</p> <h3 id="motivation">Motivation</h3> <h4 id="high-level-abstractions-vs-gpu-intricacies">High-Level Abstractions vs. GPU Intricacies</h4> <p>Many ML frameworks maintain abstraction layers (e.g., Python-level dispatch) that enable flexibility but introduce overhead. Launching numerous small GPU kernels results in underutilization of streaming multiprocessors (SMs) and frequent CPU-GPU synchronization. By writing specialized CUDA kernels that fuse multiple steps of an operation, significant speedups can be attained.</p> <h4 id="memory-management-challenges">Memory Management Challenges</h4> <p>Iterative methods often create repeated memory allocations for intermediate results, causing fragmentations that slow overall performance. Fused kernels can reuse shared memory buffers. For example, Gram-Schmidt orthogonalization and matrix multiplications in Arnoldi iterations can be done with a single pass over data in well-structured kernels that reduce overhead.</p> <h4 id="reduction-in-synchronization">Reduction in Synchronization</h4> <p>A single fused kernel launch means fewer synchronization points and fewer transitions between the CPU and GPU. These invisible costs often dominate runtime when matrix sizes grow large and the number of iterations increases.</p> <h3 id="fused-kernel">Fused Kernel</h3> <h4 id="arnoldi-iteration">Arnoldi Iteration</h4> <p>Arnoldi builds an orthonormal basis to approximate dominant eigenvalues. It requires matrix-vector multiplications and repeated Gram-Schmidt steps. A specialized fused kernel can privatize sums in shared memory, thereby avoiding multiple back-and-forth operations. Benchmarks showed up to 5x speedups compared to CoLA’s default GPU implementation, particularly when iterating over large matrices.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/speedup-480.webp 480w,/assets/img/speedup-800.webp 800w,/assets/img/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> CoLA kernels exhibit notable performance enhancements over both CoLA CUDA and CoLA CPU implementations, achieving up to 5x speedups. </div> <h4 id="cholesky-decomposition--inverse">Cholesky Decomposition &amp; Inverse</h4> <p>Cholesky factorization factors a positive definite matrix into L and Lᵀ, which can be inverted for solving linear systems more efficiently. A three-step CUDA approach: decompose_cholesky: Performs in-place diagonal updates and normalizes all relevant rows. inverse_lower: Inverts the lower triangular component. multiply_lower: Multiplies the invert to finalize the process. Although it required fewer kernel calls (only four total), careful use of cooperative groups and atomic operations meant the GPU version sometimes ran slower than PyTorch’s mixed CPU-GPU approach. However, it only required a fraction of the memory that PyTorch used.</p> <h4 id="hutchinson-trace-estimation">Hutchinson Trace Estimation</h4> <p>Hutchinson’s method is a stochastic approach to approximate the trace of large matrices. An efficient variant uses random probes paired with fused matrix-vector multiplications and incremental summations. Current prototypes show that choosing optimal batch sizes, tolerances, and iteration counts is vital, as naive settings can cause high errors or underutilized GPU resources. Tuning these parameters promises more reliable convergence.</p> <h4 id="singular-value-decomposition">Singular Value Decomposition</h4> <p>By leveraging cuBLAS and cuSOLVER APIs, SVD computations can be accelerated significantly. In this project, the main GPU cost shifted from Python-level gather/scatter operations (e.g., transpose, clone) to the actual numerical routines. Profiling indicated that specialized kernels offloaded nearly all work to symmetrical matrix-vector operations (SYMV), reducing overhead and achieving 10x or more speedups for moderately sized matrices.</p> <h3 id="observations-and-key-takeaways">Observations and Key Takeaways</h3> <ul> <li> <p>Massive Kernel Launch Overhead Profiling indicated that frameworks sometimes launch dozens of kernels for a single operation, triggering repeated device synchronization and global memory thrashing. Consolidating these operations into fewer, larger kernels removed this overhead.</p> </li> <li> <p>Memory Allocation Efficiency Merged kernels absorb intermediate steps without writing temporary buffers to global memory. This reduces fragmentation and speeds up iterative operations that rely heavily on partial results.</p> </li> <li> <p>Trade-Offs in Implementation Complexity While specialized kernels outperform high-level libraries in many cases, they increase development overhead. For instance, PyTorch uses combined CPU-GPU routines to cleverly split tasks, which can sometimes outperform pure GPU approaches for certain matrix shapes or sizes.</p> </li> <li> <p>Tailoring Kernel Parameters Fusing is beneficial but not always straightforward. Choosing the right threads-per-block, balancing shared memory usage, and atomic operations all shape performance. Each kernel (e.g., Arnoldi vs. Cholesky) has different optimal configurations.</p> </li> <li> <p>Leveraging GPU Libraries Rather than reimplementing advanced methods from scratch, integrating libraries like cuBLAS and cuSOLVER for dense operations (gemm, eigendecompositions) often offers a fast path to speedups. The biggest gains then come from carefully orchestrating these calls with custom logic to minimize overhead.</p> </li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Fusing multiple linear algebra operations into dedicated CUDA kernels can greatly reduce the kernel launch overhead and improve SM utilization. Although approaches like Arnoldi fusion achieve clear speedups, other methods (like Cholesky) highlight the nuanced interplay between heavy parallelism and algorithmic dependencies. For GPU-intensive tasks such as SVD, leveraging vendor libraries while reducing Python-level overhead has yielded significant performance benefits. There is still room for deeper optimizations—like more sophisticated kernel fusion heuristics and dynamic shape adaptations—but these initial results demonstrate that a carefully handcrafted CUDA approach can unlock considerable gains over compositional frameworks operating with off-the-shelf kernel calls.</p> <p>This project underscores the principle that high-level abstractions, while convenient, must sometimes be supplemented or replaced by specialized GPU kernels to achieve peak performance in large-scale linear algebra.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Satyanarayana Chillale. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>